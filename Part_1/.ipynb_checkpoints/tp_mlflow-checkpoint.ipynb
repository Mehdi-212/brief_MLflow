{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formation ML Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cette formation est de vous apprendre à utiliser ML Flow   \n",
    "MLflow est un dashboard pour expériences de Machine Learning.  \n",
    "Ce tp permet pas à pas comment lancer une expérience de Machine Learning et surveiller les paramètres, métriques et resultats de votre expérience grâce à ML Flow.\n",
    "\n",
    "Nous allons mettre en place plkusieurs modèles et découvrir MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow==1.29.0\n",
      "  Downloading mlflow-1.29.0-py3-none-any.whl (16.9 MB)\n",
      "     ---------------------------------------- 16.9/16.9 MB 7.4 MB/s eta 0:00:00\n",
      "Collecting numpy==1.23.4\n",
      "  Downloading numpy-1.23.4-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "     ---------------------------------------- 14.7/14.7 MB 6.5 MB/s eta 0:00:00\n",
      "Collecting pandas==1.5.1\n",
      "  Downloading pandas-1.5.1-cp39-cp39-win_amd64.whl (10.9 MB)\n",
      "     ---------------------------------------- 10.9/10.9 MB 7.1 MB/s eta 0:00:00\n",
      "Collecting scikit-learn==1.1.2\n",
      "  Downloading scikit_learn-1.1.2-cp39-cp39-win_amd64.whl (7.4 MB)\n",
      "     ---------------------------------------- 7.4/7.4 MB 8.5 MB/s eta 0:00:00\n",
      "Collecting matplotlib==3.6.0\n",
      "  Downloading matplotlib-3.6.0-cp39-cp39-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 8.1 MB/s eta 0:00:00\n",
      "Collecting seaborn==0.12.1\n",
      "  Downloading seaborn-0.12.1-py3-none-any.whl (288 kB)\n",
      "     -------------------------------------- 288.2/288.2 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting databricks-cli<1,>=0.8.7\n",
      "  Downloading databricks-cli-0.17.4.tar.gz (82 kB)\n",
      "     ---------------------------------------- 82.3/82.3 kB 4.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docker<7,>=4.0.0\n",
      "  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n",
      "     -------------------------------------- 147.5/147.5 kB 9.2 MB/s eta 0:00:00\n",
      "Collecting querystring-parser<2\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting sqlalchemy<2,>=1.4.0\n",
      "  Downloading SQLAlchemy-1.4.46-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 11.2 MB/s eta 0:00:00\n",
      "Collecting pyyaml<7,>=5.1\n",
      "  Downloading PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)\n",
      "     -------------------------------------- 151.6/151.6 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: entrypoints<1 in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from mlflow==1.29.0->-r requirements.txt (line 1)) (0.4)\n",
      "Collecting protobuf<5,>=3.12.0\n",
      "  Downloading protobuf-4.21.12-cp39-cp39-win_amd64.whl (527 kB)\n",
      "     ------------------------------------- 527.0/527.0 kB 11.0 MB/s eta 0:00:00\n",
      "Collecting cloudpickle<3\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting importlib-metadata!=4.7.0,<5,>=3.7.0\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Collecting gitpython<4,>=2.1.0\n",
      "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
      "     -------------------------------------- 184.0/184.0 kB 5.6 MB/s eta 0:00:00\n",
      "Collecting prometheus-flask-exporter<1\n",
      "  Downloading prometheus_flask_exporter-0.21.0-py3-none-any.whl (18 kB)\n",
      "Collecting alembic<2\n",
      "  Downloading alembic-1.9.3-py3-none-any.whl (210 kB)\n",
      "     ------------------------------------- 210.6/210.6 kB 13.4 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2.17.3\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting click<9,>=7.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 kB ? eta 0:00:00\n",
      "Collecting waitress<3\n",
      "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.7/57.7 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.8/42.8 kB ? eta 0:00:00\n",
      "Collecting scipy<2\n",
      "  Downloading scipy-1.10.0-cp39-cp39-win_amd64.whl (42.5 MB)\n",
      "     ---------------------------------------- 42.5/42.5 MB 4.6 MB/s eta 0:00:00\n",
      "Collecting pytz<2023\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "     -------------------------------------- 499.4/499.4 kB 7.9 MB/s eta 0:00:00\n",
      "Collecting packaging<22\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 40.8/40.8 kB ? eta 0:00:00\n",
      "Collecting Flask<3\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "     -------------------------------------- 101.5/101.5 kB 6.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from pandas==1.5.1->-r requirements.txt (line 3)) (2.8.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.0.0\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp39-cp39-win_amd64.whl (160 kB)\n",
      "     ------------------------------------- 160.2/160.2 kB 10.0 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-win_amd64.whl (55 kB)\n",
      "     ---------------------------------------- 55.4/55.4 kB ? eta 0:00:00\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.4.0-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 6.6 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "     -------------------------------------- 965.4/965.4 kB 8.8 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 98.3/98.3 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.7/78.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from click<9,>=7.0->mlflow==1.29.0->-r requirements.txt (line 1)) (0.4.6)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow==1.29.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from docker<7,>=4.0.0->mlflow==1.29.0->-r requirements.txt (line 1)) (305.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from docker<7,>=4.0.0->mlflow==1.29.0->-r requirements.txt (line 1)) (0.58.0)\n",
      "Collecting urllib3>=1.26.0\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.6/140.6 kB 8.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from Flask<3->mlflow==1.29.0->-r requirements.txt (line 1)) (3.1.2)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "     -------------------------------------- 232.7/232.7 kB 7.2 MB/s eta 0:00:00\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.7/62.7 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.12.1-py3-none-any.whl (6.7 kB)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from prometheus-flask-exporter<1->mlflow==1.29.0->-r requirements.txt (line 1)) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from requests<3,>=2.17.3->mlflow==1.29.0->-r requirements.txt (line 1)) (2022.12.7)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.0.1-cp39-cp39-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.5/96.5 kB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from requests<3,>=2.17.3->mlflow==1.29.0->-r requirements.txt (line 1)) (3.4)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp39-cp39-win_amd64.whl (192 kB)\n",
      "     -------------------------------------- 192.1/192.1 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\utilisateur\\anaconda3\\envs\\brief_mlops\\lib\\site-packages (from Jinja2>=3.0->Flask<3->mlflow==1.29.0->-r requirements.txt (line 1)) (2.1.1)\n",
      "Building wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.17.4-py3-none-any.whl size=142875 sha256=2a5fd64b8f693fdbaf35f70519e8e4dcaf8e7fac2dcd951cf9f59e287a042f0d\n",
      "  Stored in directory: c:\\users\\utilisateur\\appdata\\local\\pip\\cache\\wheels\\69\\79\\af\\5dedd3bac0031e64e081bd37505d5d55d9cd566fbad743d259\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: pytz, charset-normalizer, zipp, Werkzeug, waitress, urllib3, threadpoolctl, tabulate, sqlparse, smmap, querystring-parser, pyyaml, pyparsing, pyjwt, protobuf, pillow, oauthlib, numpy, Mako, kiwisolver, joblib, itsdangerous, greenlet, fonttools, cycler, cloudpickle, click, sqlalchemy, scipy, requests, pandas, packaging, importlib-metadata, gitdb, contourpy, scikit-learn, matplotlib, gitpython, Flask, docker, databricks-cli, alembic, seaborn, prometheus-flask-exporter, mlflow\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 22.0\n",
      "    Uninstalling packaging-22.0:\n",
      "      Successfully uninstalled packaging-22.0\n",
      "Successfully installed Flask-2.2.2 Mako-1.2.4 Werkzeug-2.2.2 alembic-1.9.3 charset-normalizer-3.0.1 click-8.1.3 cloudpickle-2.2.1 contourpy-1.0.7 cycler-0.11.0 databricks-cli-0.17.4 docker-6.0.1 fonttools-4.38.0 gitdb-4.0.10 gitpython-3.1.30 greenlet-2.0.2 importlib-metadata-4.13.0 itsdangerous-2.1.2 joblib-1.2.0 kiwisolver-1.4.4 matplotlib-3.6.0 mlflow-1.29.0 numpy-1.23.4 oauthlib-3.2.2 packaging-21.3 pandas-1.5.1 pillow-9.4.0 prometheus-flask-exporter-0.21.0 protobuf-4.21.12 pyjwt-2.6.0 pyparsing-3.0.9 pytz-2022.7.1 pyyaml-6.0 querystring-parser-1.2.4 requests-2.28.2 scikit-learn-1.1.2 scipy-1.10.0 seaborn-0.12.1 smmap-5.0.0 sqlalchemy-1.4.46 sqlparse-0.4.3 tabulate-0.9.0 threadpoolctl-3.1.0 urllib3-1.26.14 waitress-2.1.2 zipp-3.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow, math, time\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ouverture et visualisation du dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permet d'avoir accès au dataset une fois qu'il sera chargé\n",
    "dataset_columns = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "\n",
    "try: \n",
    "    dataset = pd.read_csv('iris.csv')\n",
    "except FileNotFoundError:\n",
    "    iris_data = datasets.load_iris(as_frame=True)\n",
    "    dataset = iris_data.get('data')\n",
    "    dataset.columns = columns=dataset_columns\n",
    "    dataset[\"target\"] = iris_data.get('target')\n",
    "    dataset['target'] = dataset['target'].replace(dataset['target'].unique().tolist(), iris_data.get('target_names'))\n",
    "    dataset.to_csv('iris.csv', index=False)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichons en une simple ligne de code les caractéristiques des colonnes numériques\n",
    "\n",
    "#Code ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifions que les données soient bien homogènes (2 méthodes)\n",
    "\n",
    "# Code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons maintenant le nuage de points de chaque paire de caractéristiques. Sur la diagonale, on retrouve un histogramme du nombre d'échantillons selon la valeur de la caractéristique. On peut remarquer certaines structures, par exemple des groupes, dans les relations entre certaines caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tout groupe confondu, affichez le nuage de point par paire de variable et en diagonale l'histogramme de la variable \n",
    "pd.plotting.scatter_matrix(dataset,figsize=(12,8))\n",
    "plt.suptitle('Scatter matrix of the Iris dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#essayons avec seaborn, et mettons une couleur par classe\n",
    "\n",
    "#Code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque des corrélations avec certaines caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ML Flow : premiers pas avec un modèle aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un premier exemple prenons une classification 'simple' : le modèle aléatoire. Ce modèle attribuera aléatoirement une classe à chaque échantillon du dataset. \n",
    "\n",
    "Séparons le dataset en deux X, nos features et Y notre class des iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.loc[:,dataset_columns].values\n",
    "Y = dataset.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construisons notre modèle aléatoire :\n",
    "dummy = DummyClassifier(strategy=\"uniform\")\n",
    "dummy.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons ses prédictions pour les premiers échantillons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = dummy.predict(X)\n",
    "for i in range(5):\n",
    "    print(X[i], ', vraie classe : ', Y[i], ', prédiction : ', Y_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vous laisse imagienr la qualité du modèle sachant qu'il prédit ce qu'il apprend ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est maintenant que MLflow rentre en jeu, enregistrons les performances du modèle  \n",
    "Histoire de ne pas avoir d'erreur créons un fichier `iris.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch iris.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='Modele_Aleatoire'):\n",
    "    mlflow.log_param('Je teste mon modèle', 'DummyClassifier')\n",
    "    mlflow.log_metric('Accuracy', accuracy_score(Y, Y_pred))\n",
    "    mlflow.log_artifact('iris.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='Modele_Aleatoire avec csv'):\n",
    "    mlflow.log_param('Je teste mon modèle', 'DummyClassifier')\n",
    "    mlflow.log_metric('Accuracy', accuracy_score(Y, Y_pred))\n",
    "    mlflow.log_artifact('iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ici on a remplacé le fichier vide `iris.txt` par `iris.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si jamais vous avez supprimé votre dossier `mlruns` relancez le notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAUSE ! c'est là ou ça devient intéressant, vous venez de créer un nouveau dossier `mlruns`, il est nécessaire pour la suite des évènements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Toujours dans le dossier formation-mlflow, entrez la commande mlflow ui. Le message Serving on http://XXXXXXX:5000 devrait s'afficher. Ouvrez maintenant un navigateur web et ouvrez une fenêtre vers l'url http://localhost:5000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ca y est les resultats sont visibles sur l'interface ! discutons en un peu\n",
    "\n",
    "ML Flow permet d'enregistrer puis visualiser 4 types de choses :\n",
    "\n",
    "des paramètres : ce sont des valeurs (int, float, string, ...) qui ne varient pas au cours d'un 'run', ici `MODEL_NAME` ou encore `je teste mon modèle` est un paramètre   \n",
    "\n",
    "des métriques : ce sont des valeurs numériques qui peuvent varier au cours du 'run', ici Accuracy est une métrique   \n",
    "\n",
    "des fichiers : ces fichiers peuvent prendre n'importe quelle forme (png, jpeg, gif, txt, ...) et ne sont pas modifiables au cours du 'run. Ici nous avons fait une copie du fichier iris.csv et de iris.txt   \n",
    "\n",
    "Toutes ces variables sont regroupées dans un même 'run', ici nommé `Modele_Aleatoire` ou `modele aleatoire avec csv`. Un 'run' correspond à une ligne du tableau. Les 'runs' peuvent être regroupés en 'experiences', ici nommée Default dans la colonne de gauche. Nous verons comment changer le nom de l'expérience par la suite  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Création du 'run' ML FLow, nommé ici 'Modele_Aleatoire'\n",
    "#    un run est inclus dans une expérience\n",
    "with mlflow.start_run(run_name='Modele_Aleatoire'):\n",
    "\n",
    "    # 2. Enregistrement d'un paramètre, c'est par exemple le nom de notre modèle testé, les fameux 'final1', 'final_final'\n",
    "    mlflow.log_param('MODEL_NAME', 'DummyClassifier')\n",
    "\n",
    "    # 3. Enregistrement d'une métrique, pour qu'on n'ait plus besoin de l'ecrire sur un cahier ou sur excel\n",
    "    mlflow.log_metric('Accuracy', accuracy_score(Y, Y_pred))\n",
    "\n",
    "    # 4. Enregistrement d'un fichier, c'est notre artefact, ce qui permet aussi le reproductibilité\n",
    "    mlflow.log_artifact('iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice d'application  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "petal width est une variable très discriminante pour classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(dataset, x='petal width', y='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc essayer de coder notre propre modèle de classification selon cette spécificité et enregistrer ses performances avec ML FLow.\n",
    "\n",
    "Pour cela, commencez par coder une fonction qui :\n",
    "\n",
    "prend en entrée une liste d'échantillons  \n",
    "retourne la liste des classes des échantillons  \n",
    "afin de retourner la classe de façon automatisée crée un quantile à 0.33 et un à 0.66, les classes seront dans l'ordre suivant :  \n",
    "- setosa, versicolor, virginica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_classification_model(list_samples):\n",
    "    \"\"\"\n",
    "    list_sample represente l'ensemble des feature\n",
    "    quantile est une liste comme celle ci : [elem1, elem2] \n",
    "    \"\"\"\n",
    "    #Code ici\n",
    "    return petal_width_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions ici que notre modele fonctionne sur quelques échantillons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_cond = simple_classification_model(X)\n",
    "for i in range(5):\n",
    "    print(X[i], ', vraie classe : ', Y[i], ', prédiction : ', Y_pred_cond[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilisons à nouveau MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ML Flow : Comparaison de plusieurs modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est là ou MLflow devient très intéressant ! il permet de centraliser les experiences afin d'améliorer les comparaisons.  \n",
    "nous allons créer plusieurs modèles très simples, pour se faire séparons notre dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.20, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisons la liste des modèles à tester :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LogisticRegression', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNeighborsClassifier', KNeighborsClassifier()))\n",
    "models.append(('DecisionTreeClassifier', DecisionTreeClassifier()))\n",
    "models.append(('GaussianNB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajoutons cette fonction qui va nous permettre de tracer de jolies matrices de confusion par la suite :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names, title, \n",
    "                          normalize=True, save_path='matrix.png'):\n",
    "    import itertools\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    cmap = plt.get_cmap('Blues')\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'\\\n",
    "               .format(accuracy, misclass))\n",
    "    plt.gcf().canvas.draw()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant entraîner nos modèles et enregistrer leurs paramètres et métriques avec ML Flow.\n",
    "\n",
    "Nous allons voir deux nouvelles choses :\n",
    "\n",
    "Comment créer une nouvelle expérience\n",
    "Comment enregistrer plusieurs paramètres ou métriques d'un seul coup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avant cela nous allons reconstruire le dataset pour savoir le train et le test\n",
    "def artifact_df(X_training, Y_training, X_valid, Y_valid):\n",
    "    df_train = pd.DataFrame(X_training, columns=dataset_columns)\n",
    "    df_train['target'] = Y_training\n",
    "    df_train.to_csv('train_data.csv')\n",
    "\n",
    "    df_val = pd.DataFrame(X_valid, columns=dataset_columns)\n",
    "    df_val['target'] = Y_valid\n",
    "    df_val.to_csv('validation_data.csv')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Création d'une nouvelle expérience\n",
    "#    Tous les nouveaux runs seront enregistrés dans cette expérience\n",
    "mlflow.set_experiment('Compare_models')\n",
    "\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "\n",
    "    # 2. Création et début d'un nouveau run\n",
    "    with mlflow.start_run(run_name=name):\n",
    "\n",
    "        # 3. Enregistrement de plusieurs paramètres sous forme d'un dictionnaire\n",
    "        params = {}\n",
    "        params['MODEL_NAME'] = name\n",
    "        params['TRAIN_SIZE'] = len(X_train)\n",
    "        params['TEST_SIZE'] = len(X_val)\n",
    "        params['SEED'] = random_seed\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # On note le moment du début de l'expérience pour mesurer la durée de l'entraînement\n",
    "        start = time.time()\n",
    "\n",
    "        # Entraînement du modèle\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        training_time = time.time() - start\n",
    "\n",
    "        predictions = model.predict(X_val)\n",
    "\n",
    "        # 4. Enregistrement de plusieurs métriques sous forme d'un dictionnaire\n",
    "        metrics = {}\n",
    "        metrics['Accuracy'] = accuracy_score(Y_val, predictions)\n",
    "        metrics['Precision'] = precision_score(Y_val, predictions, average='macro')\n",
    "        metrics['Recall'] = recall_score(Y_val, predictions, average='macro')\n",
    "        metrics['Training Time'] = training_time\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # 5. Enregistrement de la matrice de confusion\n",
    "        cm = confusion_matrix(Y_val, predictions)\n",
    "        plot_confusion_matrix(cm, ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'],\n",
    "                              'Confusion matrix '+name)\n",
    "        mlflow.log_artifact('matrix.png')\n",
    "        artifact_df(X_train, Y_train, X_val, Y_val)\n",
    "        mlflow.log_artifact('train_data.csv')\n",
    "        mlflow.log_artifact('validation_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la partie gauche de l'UI vous trouverez 2 experiences differentes, selectionnez `Compare_models` et pour chaque run vous aurez la matrice de confusion et le train, test associé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice d'application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilisez la fonction qui classifie de façon conditionnelle les iris dans l'experience crée juste avant  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('Compare_models')\n",
    "\n",
    "with mlflow.start_run(run_name='My_Conditional_Model'):\n",
    "\n",
    "    #Code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ML Flow : Enregistrer l'évolution d'une métrique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette dernière partie, nous allons voir comment enregistrer l'évolution d'une métrique au cours du temps. Pour cela, nous allons utiliser comme modèle un réseau de neurones basique : le perceptron multi-couches.\n",
    "\n",
    "Afin de calculer nos métriques au cours de l'entrainement, nous decoupons celui-ci en plusieurs epochs. A chaque epoch, le reseau de neurone s'entraîne une fois sur l'intégralité du dataset. Tout ceci est fait automatiquement dans la fonction fit par défaut de scikit learn, mais nous allons faire les choses manuellement ici, pour visualiser l'évolution des métriques au cours de l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Création et début d'un nouveau run\n",
    "with mlflow.start_run(run_name='Multilayer Perceptron'):\n",
    "\n",
    "    model = MLPClassifier(max_iter=10)\n",
    "    mlflow.log_param('MODEL_NAME', 'MLPClassifier')\n",
    "\n",
    "    N_TRAIN_SAMPLES = X_train.shape[0]\n",
    "    N_EPOCHS = 10\n",
    "    N_BATCH = 8\n",
    "    N_CLASSES = np.unique(Y_train)\n",
    "    scores_train = []\n",
    "    scores_test = []\n",
    "\n",
    "    # EPOCH\n",
    "    epoch = 0\n",
    "    while epoch < N_EPOCHS:\n",
    "        if epoch % 2 == 0 : print('Epoch: ', epoch)\n",
    "        # SHUFFLING\n",
    "        random_perm = np.random.permutation(X_train.shape[0])\n",
    "        mini_batch_index = 0\n",
    "        while True:\n",
    "            # TRAIN ON MINI-BATCH\n",
    "            indices = random_perm[mini_batch_index:mini_batch_index + N_BATCH]\n",
    "            model.partial_fit(X_train[indices], Y_train[indices], classes=N_CLASSES)\n",
    "            mini_batch_index += N_BATCH\n",
    "            if mini_batch_index >= N_TRAIN_SAMPLES:\n",
    "                break\n",
    "\n",
    "        metrics = {}\n",
    "        predictions = model.predict(X_val)\n",
    "        metrics['Accuracy'] = accuracy_score(Y_val, predictions)\n",
    "        metrics['Precision'] = precision_score(Y_val, predictions, average='macro')\n",
    "        metrics['Recall'] = recall_score(Y_val, predictions, average='macro')\n",
    "        metrics['Loss'] = model.loss_\n",
    "        # 2. Enregistrement de plusieurs métriques pour une epoch donnée\n",
    "        mlflow.log_metrics(metrics, step=epoch)\n",
    "        time.sleep(3)\n",
    "\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revenez sur le tableau des runs dans le dashboard et ouvrez le run que nous venons de créer. En cliquant sur l'une des métriques, vous pourrez voir son évolution au cours de l'apprentissage du réseau de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (OPTIONAL\n",
    ") 5. Un exercice d'application des temps d'attente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pour finir, je vous propose un exercice d'application sur le thème d'un parc d'attraction, qui regroupera tout ce que nous avons appris ici.\n",
    "\n",
    "Pour cela nous allons étudier les temps d'attente de 3 attractions sur 10 jours d'avril  \n",
    "FYI : les données on été choquées avec une uniforme pour qu'elles ne représentent pas totalement la réalité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_df = pd.read_csv('wait.csv')\n",
    "wait_df = wait_df.sort_values(\"start\").reset_index()\n",
    "# on enleve les 10 derniers elements de chaque groupe\n",
    "result1 = wait_df.drop(wait_df.groupby('attraction').tail(10).index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wait_df.drop_duplicates()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(df.start, df.waiting_time)\n",
    "plt.title(\"temps d'attente sur une attraction en avril 2022\")\n",
    "plt.ylabel('temps (min)')\n",
    "plt.xticks(df.start.values[::300], rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez au moins 3 modèles de prévisions des temps d'attente pour `l'attraction_2` uniquement et sur les 30 dernières periodes du dataframe disponible\n",
    "Ajoutez au moins 3 métriques pour évaluer les performances de vos modèles  \n",
    "Pour chaque métrique, évaluez là sur l'ensemble de votre prévision, mais aussi sur chacune des échéances (T1, ..., Tk) afin de tracer dans MLFlow la courbe d'évolution de cette métrique.  \n",
    "Enregistrez tout cela dans MLFlow, dans une nouvelle expérience.  \n",
    "conseil : persistence, lag et forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AJOUTEZ VOTRE CODE ICI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d56449fddd92a32e29a3fa1d3dd6cedd3c8f80e8884ba77135626f09367c4ca8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
